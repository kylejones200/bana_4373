{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izJch3vfcs0a"
   },
   "source": [
    "# Deep Learning and Neural Networks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEg4H8vN_yBo"
   },
   "source": [
    "## Deep learning / Deep neural network\n",
    "\n",
    "Deep learning is a group of machine learning algorithms that specifically focuses on utilizing neural network architecture.\n",
    "The neural network approach attempt to mimic the function of biological neurons by stacking layers of neuron units that can pass information.\n",
    "The network will pass the inforamtion back and forth to adjust the weights of neuron units using backpropagation algorithm based on the objective function (loss function, criterion) that we provide.\n",
    "\n",
    "The advantages of using deep learning approach includes:\n",
    "- Modularization\n",
    "- Universality theorem (one hidden layer is enough, but much effective while deeper)\n",
    "- End-to-end learning (less feature engineering---if you don't know what features are important at all)\n",
    "\n",
    "![adv](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/adv.png)\n",
    "\n",
    "[Source] Courtesy by Prof. HY Lee (NTU)\n",
    "\n",
    "However, there are still some cons of using deep neural networks such as interpretability and heavy computation.\n",
    "\n",
    "As a promising algorithmic approach for predictive analytics (and also the hype of deep learning), we would like to introduce you how to use deep neural networks for clinical machine learning problems.\n",
    "\n",
    "The mathematical details of neural network approach will not be emphasized in the tutorial.\n",
    "The foundation of deep neural network is using **gradient descent** to update parameters, and all gradients are computed by **backpropagation algorithm**.\n",
    "Please refer to [Deep Learning book by Goodfellow et al.](http:/www.deeplearningbook.org/) for detailed information.  \n",
    "\n",
    "To build the neural network without too much efforts in python, we adopt the high-level python deep learning API, **Keras**, to train neural networks.\n",
    "\n",
    "After going through this tutorial, we hope that you will understand how to use keras to design and build simple neural network for classification problems, and how to evaluate the keras neural network models.\n",
    "\n",
    "In this tutorial, we directly use the most complicated data, PhysioNet dataset, as an example to demonstrate the performance of deep learning models.\n",
    "Please refer to tutorial Part I for all details of data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unXxMfScnBWh",
    "outputId": "51f11ec6-2e7c-4db8-e60f-ff3a2aa022a1"
   },
   "outputs": [],
   "source": [
    "! pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh-d9eYuCr1x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/data/PhysionetChallenge2012_data.csv')\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FevajKnfDnJ6"
   },
   "source": [
    "Before training neural network, we need to do some transformations for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ge6DRpYGEYAH"
   },
   "outputs": [],
   "source": [
    "X = Imputer(missing_values=np.nan, strategy='mean').fit(X).transform(X)\n",
    "X = StandardScaler().fit(X).transform(X)\n",
    "\n",
    "# Encoding Categorical Data\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKcLH75tuGRw"
   },
   "source": [
    "Next, we import keras modules and construct a very simple three-layer neural network architecture with input, hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import Image\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variants of deep neural networks.\n",
    "\n",
    "![nn](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/nn.png)\n",
    "\n",
    "[Source] Deep Learning, Chapter 1 (Goodfellow, 2016)\n",
    "\n",
    "We will start from the simplest one, feedforward neural network, which is similar to the neural network architecture shown in the following figure.\n",
    "\n",
    "![nn](https://cdn-images-1.medium.com/max/1600/1*QVIyc5HnGDWTNX3m-nIm9w.png)\n",
    "\n",
    "[Source] https://medium.com/@curiousily/tensorflow-for-hackers-part-iv-neural-network-from-scratch-1a4f504dfa8\n",
    "\n",
    "In our first neural network model, we will construct the one that\n",
    "\n",
    "- take the input and pass them into the 64-dimension first hidden layer, \n",
    "- take the output of the first layer and pass them into the  8-dimension second layer,\n",
    "- take the output of the second layer and pass them into the last layer for prediction,\n",
    "- the output of the last layer is the prediction.\n",
    "\n",
    "One more hidden layer than the above figure.\n",
    "\n",
    "In keras, we use `Sequential()` as the skeleton of the neural network model, and sequentially add the layer on it.\n",
    "After building the layers, we need to compile the model and defined the optimizer, loss function and evaluation metrics to optimize our model.\n",
    "In this example, we use the optimizer called `adam`, to minimize the value of loss function `binary_crossentropy` (if you work on the regression problem, remember to change to `mse`), and judge by accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "kK615g3AErss",
    "outputId": "2ec7f3e9-7d97-4f26-ff8e-d905a427ca1d"
   },
   "outputs": [],
   "source": [
    "# initialize neural network\n",
    "clf = Sequential()\n",
    "\n",
    "# first hidden layer for input data\n",
    "clf.add(Dense(units=64, \n",
    "              kernel_initializer='uniform',\n",
    "              activation='relu', \n",
    "              input_dim=X.shape[1]))\n",
    "\n",
    "# second hidden layer\n",
    "clf.add(Dense(units=8, \n",
    "              kernel_initializer='uniform', \n",
    "              activation='relu'))\n",
    "\n",
    "# the last hidden layer for output\n",
    "clf.add(Dense(units=1, \n",
    "              kernel_initializer='uniform', \n",
    "              activation='sigmoid'))\n",
    "\n",
    "# compile the network\n",
    "clf.compile(optimizer='adam', \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(clf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3dHEHTDWvy8"
   },
   "source": [
    "Then, it's time to train the model!\n",
    "Smaller batch size usually yields better performance, yet with slow speed.\n",
    "The epoch size of $n$ means that the algorithm will go through whole dataset $n$ times.\n",
    "\n",
    "After training, we visualize the training history and see how the accuracy and loss went during the process.\n",
    "We also use confusion matrix and **pycm** for computing the performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlPbkMOpVWPN",
    "outputId": "b91ec768-4837-461b-cfa1-02837d288a6a"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = clf.fit(x=X_train, y=y_train,\n",
    "                    validation_split=0.2,  \n",
    "                    batch_size=100,\n",
    "                    epochs=20, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "M_DCM8ZzEupD",
    "outputId": "88fe133e-c44a-41ff-8733-74da0db0c11b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "def show_train_history(train_history, train, validation):  \n",
    "    plt.plot(train_history.history[train])   \n",
    "    plt.plot(train_history.history[validation])  \n",
    "    plt.title('Train History')  \n",
    "    plt.ylabel(train)  \n",
    "    plt.xlabel('Epoch')  \n",
    "    plt.legend(['train', 'validation'], loc='upper left')  \n",
    "    plt.show() \n",
    "    \n",
    "show_train_history(history, 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDwBbG8UEwek",
    "outputId": "c33bbf90-7730-4a63-fafc-95696c5d68f7"
   },
   "outputs": [],
   "source": [
    "# we use the trained model to predict the label of test set\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = [1 if i[0] > 0.5 else 0 for i in y_pred]\n",
    "\n",
    "# show the performance using confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kasfl-8OGTUj",
    "outputId": "c0f66948-96c2-4fd7-cfd2-e7d8fb26aa48"
   },
   "outputs": [],
   "source": [
    "# we use pycm to show the performance\n",
    "\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "cm = ConfusionMatrix(actual_vector=y_test, predict_vector=y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMkAhjr6BdKl"
   },
   "source": [
    "After seeing a simple example, we discuss more about the network.\n",
    "Normally, when building a neural network, we will do the following steps:\n",
    "\n",
    "### Define a set of function (Network architecture)\n",
    "We can see that the neuron unit can be regarded as a logistic regression-like operation.\n",
    "In general, the unit will do `activation_function(weight * input + bias)` computation.\n",
    "\n",
    "![ff](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/ff.png)\n",
    "\n",
    "[Source] Courtesy by Prof. HY Lee (NTU)\n",
    "\n",
    "In the feedforward model, the layer function we used is `Dense` (fully-connected layer).\n",
    "For the `Dense` layer, you need to assign the input size, output size (`units`), and the activation function.\n",
    "For activation function, you can try the following options: `softplus`, `softsign`, `relu`, `tanh`, `sigmoid`, `hard_sigmoid`, `linear`, depends on your needs.\n",
    "However, `relu` are the most commonly used functions for intermediate layers, and `sigmoid`/`softmax` for the last layer (binary/multiclass).\n",
    "\n",
    "Multiple hidden layers are good, but there is no theorerical consensus how many layers / how many neurons / what hyperparameters you need to use to get the best performance. (That's why some people think that deep learning approach has become [alchemy](https://www.reddit.com/r/compsci/comments/7jiipp/lecun_vs_rahimi_has_machine_learning_become/))\n",
    "\n",
    "![stack](https://1.bp.blogspot.com/-gZ7UBZ-wiBE/WZU12kj6wDI/AAAAAAAAKNQ/U8998PG8U30fgzZUWVtQhzl5fpVlCxnSwCLcBGAs/s1600/layers.jpg))\n",
    "\n",
    "[Source] http://www.cl.cam.ac.uk/~pv273/slides/LOSSlides.pdf\n",
    "\n",
    "### Goodness of function (Define loss)\n",
    "The goal of prediction is to minimize the difference between the predicted value and the ground truth label. \n",
    "We need to decide how to compute the difference between them, we call this \"objective\" in deep learning.\n",
    "\n",
    "![loss](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/loss.png)\n",
    "\n",
    "[Source] Courtesy by Prof. HY Lee (NTU)\n",
    "\n",
    "In the first case, we will use `binary_crossentropy` for the binary classification task. \n",
    "For more objectives, please refer to the [keras document](https://keras.io/objectives/).\n",
    "\n",
    "### Choose the best function (Optimization and parameter setting)\n",
    "We also need to decide which optimizer will be used in the network. \n",
    "The commonly used optimizers include: `SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam`.\n",
    "The backpropagation algorithm will use the selected optimizer to minimize the objective (loss) function we defined previously.\n",
    "Finally, we need to decide the parameters `batch_size` and `epochs` for training. \n",
    "Batch size influences the speed and performance so you have to tune it and do some experiments.\n",
    "If you have $1000$ training samples and set `batch_size=50`, then in one epoch (going through all training samples once) you need to update parameters for $\\frac{1000}{50}=20$ times, until all \"mini-batches\" are picked.\n",
    "\n",
    "For prediction, you can simply use `model.evaluate()` to get the performance metrics, and use `model.predict()` to obtain the predicted results.\n",
    "\n",
    "### (optional) Exercise of regularization\n",
    "Regularization is a pratical way to prevent from overfitting problem. \n",
    "You may try to add `BatchNormalization` (batch normalization) or `Dropout` (dropout) layers and see whether they will be helpful for modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1r0BWxzyXea"
   },
   "source": [
    "\n",
    "## Exercise\n",
    "\n",
    "Now try to play with the breast cancer dataset and see whether deep neural network works well on the smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MPBtBuZTIJbx",
    "outputId": "d43caa47-fb9f-4a87-ba6e-f4de990b39a1"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "df_bc = datasets.load_breast_cancer()\n",
    "X = df_bc['data']\n",
    "y = df_bc['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "clf = Sequential()\n",
    "clf.add(Dense(units=16, \n",
    "              kernel_initializer='uniform',\n",
    "              activation='relu', \n",
    "              input_dim=X.shape[1]))\n",
    "clf.add(Dense(units=16, \n",
    "              kernel_initializer='uniform', \n",
    "              activation='relu'))\n",
    "clf.add(Dense(units=1, \n",
    "              kernel_initializer='uniform', \n",
    "              activation='sigmoid'))\n",
    "clf.compile(optimizer='adam', \n",
    "            loss='binary_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n",
    "history = clf.fit(x=X_train, y=y_train,\n",
    "                    validation_split=0.2,  \n",
    "                    batch_size=100,\n",
    "                    epochs=50, \n",
    "                    verbose=1)\n",
    "\n",
    "show_train_history(history, 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'loss', 'val_loss')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred = [1 if i[0] > 0.5 else 0 for i in y_pred]\n",
    "\n",
    "cm = ConfusionMatrix(actual_vector=y_test, predict_vector=y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgwFOuGah7KL"
   },
   "source": [
    "## Convolutional neural network (CNN) for image data\n",
    "\n",
    "Sometimes we will use image instead of structured tabular data for clinical data predictive analytics.\n",
    "Here we show a very simple image classification workflow using [MNIST handwriting database](https://en.wikipedia.org/wiki/MNIST_database) since there is no small but suitable medical image dataset for simple demonstration with deep neural network.\n",
    "\n",
    "Although not business focused, MNIST is already included in the `keras` and we can simply load it through the `keras` provided functions.\n",
    "But we still need to do reshape the data to the proper format, and normalize them before feed into the model.\n",
    "\n",
    "First, we will just simply use the feedforward neural network for image classification (10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9jbyLh-iCR4",
    "outputId": "2163918d-7efc-4a62-f734-d52dc957f20b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# load MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the data\n",
    "X_train = X_train.reshape(60000, 28*28).astype('float32')\n",
    "X_test = X_test.reshape(10000, 28*28).astype('float32')\n",
    "  \n",
    "# normalize the data\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# for keras classification, we need to use `to_categorical` to transform the label to appropriate format\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "a4olgmYvoi-A",
    "outputId": "629b22b8-b3b0-4c11-b01a-eeea31eb8b5b"
   },
   "outputs": [],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(units=256, \n",
    "                input_dim=784, \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu'))\n",
    "clf.add(Dense(units=256, \n",
    "                input_dim=64, \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu'))\n",
    "clf.add(Dense(units=10, \n",
    "                kernel_initializer='normal', \n",
    "                activation='softmax'))\n",
    "print(clf.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "dg1lEEyXZUJn",
    "outputId": "857ad702-43c2-45ab-9019-ee0b8b608295"
   },
   "outputs": [],
   "source": [
    "clf.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = clf.fit(x=X_train, y=y_train, \n",
    "                    validation_split=0.2,  \n",
    "                    batch_size=200, \n",
    "                    epochs=10,\n",
    "                    verbose=2)\n",
    "\n",
    "show_train_history(history, 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'loss', 'val_loss')\n",
    "\n",
    "scores = clf.evaluate(X_test, y_test)  \n",
    "print(\"Accuracy of testing data = {:2.1f}%\".format(scores[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWoLH9FGZvEP"
   },
   "source": [
    "We can see that feedforward neural network can do a very good job on image classification. \n",
    "However, we may also use **convolutional neural network** (CNN), to do classification considering spatial information.\n",
    "\n",
    "The key steps in CNN are:\n",
    "\n",
    "- Convolution\n",
    "- Pooling\n",
    "\n",
    "The idea of convolution is to have **sparse connectivity** (each neuron only connects to parts of the previous layer) and **parameter sharing** (neurons can use the same set of parameters). \n",
    "\n",
    "![conv](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/conv.png)\n",
    "\n",
    "[Source] Deep Learning, Chapter 9 (Goodfellow, 2016)\n",
    "\n",
    "The pooling strategy also group the neurons corresponding to the same filter with nearby receptive fields.\n",
    "\n",
    "![pool](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/pool.png)\n",
    "\n",
    "[Source] Deep Learning, Chapter 9 (Goodfellow, 2016)\n",
    "\n",
    "These characteristics preserve the spatial information, as well as reduce the model size significantly.\n",
    "\n",
    "The following figure demonstrates the process of CNN on MNIST data.\n",
    "\n",
    "![cnn](https://codetolight.files.wordpress.com/2017/11/network.png)\n",
    "\n",
    "[Source] https://codetolight.files.wordpress.com/2017/11/network.png\n",
    "\n",
    "In the following codes, we will use `keras` to build the same model and see how the performance goes using CNN on MNIST.\n",
    "You may compare the value of the figure and those in the sequential model below.\n",
    "\n",
    "We can see that the performance significantly increase using CNN approach for image data.\n",
    "\n",
    "You can also play with changing the value of parameters such as `filters`,  `activation`,  `pool_size`,  `drop_out`, `Dense` and see how the performance will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bOsDSqzho4IF",
    "outputId": "b246a500-981a-427a-89cd-6570f6f1d9d5"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Conv2D, MaxPool2D \n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  \n",
    "  \n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')  \n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "X_train = X_train / 255  \n",
    "X_test = X_test /255  \n",
    "  \n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "clf = Sequential()  \n",
    "clf.add(Conv2D(filters=10,  \n",
    "                 kernel_size=(5, 5),  \n",
    "                 padding='same',  \n",
    "                 input_shape=(28, 28, 1),  \n",
    "                 activation='relu',\n",
    "                 name='conv2d_1')) \n",
    "clf.add(MaxPool2D(pool_size=(2,2), \n",
    "                    name='max_pooling2d_1'))  \n",
    "clf.add(Conv2D(filters=20,  \n",
    "                 kernel_size=(5, 5),  \n",
    "                 padding='same',  \n",
    "                 input_shape=(28, 28, 1),  \n",
    "                 activation='relu',\n",
    "                 name='conv2d_2'))  \n",
    "clf.add(MaxPool2D(pool_size=(2,2), \n",
    "                    name='max_pooling2d_2'))  \n",
    "clf.add(Dropout(0.25, name='dropout_1'))\n",
    "clf.add(Flatten(name='flatten_1'))\n",
    "clf.add(Dense(100, activation='relu', name='dense_1'))  \n",
    "#clf.add(Dropout(0.5, name='dropout_2'))\n",
    "clf.add(Dense(10, activation='softmax', name='dense_2'))\n",
    "print(clf.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "4-mgNDeXascS",
    "outputId": "c9d558e2-81f5-49c1-f1f9-64609a83cf48"
   },
   "outputs": [],
   "source": [
    "clf.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])  \n",
    "  \n",
    "history = clf.fit(x=X_train, y=y_train,\n",
    "                    validation_split=0.2,  \n",
    "                    batch_size=200,\n",
    "                    epochs=10, \n",
    "                    verbose=1)\n",
    "\n",
    "show_train_history(history, 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'loss', 'val_loss')\n",
    "\n",
    "scores = clf.evaluate(X_test, y_test)  \n",
    "print(\"Accuracy of testing data = {:2.1f}%\".format(scores[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgpjjElhiC2m"
   },
   "source": [
    "## Recurrent neural network (RNN) for sequential data / text\n",
    "\n",
    "Text data are even much important for clinical data predictive analytics since they usually include a lot of information that are not saved in the tabular data.\n",
    "However, preprocessing text data is quite messy and time-consuming (but extremely important).\n",
    "We won't cover this part in the tutorial but only quickly go through the modeling.\n",
    "\n",
    "We again show a simple text sentiment classification workflow using [IMDB Movie reviews sentiment classification database](https://keras.io/datasets/), which is already preprocessed and labeled.\n",
    "\n",
    "For your own text data, please make sure that they are in the same format as the IMDB dataset in `keras`.\n",
    "\n",
    "For text data, you will use a type of neural network, **recurrent neural network** (RNN) for the sequential information. \n",
    "RNN and its variants can be used for different kinds of sequential data such as text, speech, audio or time-series data.\n",
    "\n",
    "RNN has the feature of memorizing the previous input while training through passing the hidden state to the neural network again and again---which is suitable for sequential modeling.\n",
    "\n",
    "![rnn](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/rnn.png)\n",
    "\n",
    "[Source] Deep Learning, Chapter 10 (Goodfellow, 2016)\n",
    "\n",
    "Here we just demonstrate two approaches of modeling the IMDB data:\n",
    "\n",
    "- using a RNN variant, long short term memory neural network (LSTM) for modeling,\n",
    "- combining CNN and LSTM for modeling (CNN for compressing the sequence information).\n",
    "\n",
    "For the detailed differences of neural network architecture, please check the figure of neural networks below.\n",
    "\n",
    "To make the modeling process simpler, we make all sequences (texts) the same length using zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "id": "JoCgy6YGiF3U",
    "outputId": "b6b3254d-148b-43c0-bc82-9923ab1c2546"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "top_words = 3000\n",
    "max_review_length = 200\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# LSTM\n",
    "clf = Sequential()\n",
    "clf.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "clf.add(LSTM(100))\n",
    "clf.add(Dense(1, activation='sigmoid'))\n",
    "clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "_GFrLen-dwb1",
    "outputId": "a6019445-4f68-49cc-aca4-bb2302d5ec65"
   },
   "outputs": [],
   "source": [
    "history = clf.fit(x=X_train, y=y_train,\n",
    "                    validation_split=0.2,  \n",
    "                    batch_size=200,\n",
    "                    epochs=10, \n",
    "                    verbose=1)\n",
    "\n",
    "show_train_history(history, 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'loss', 'val_loss')\n",
    "\n",
    "scores = clf.evaluate(X_test, y_test)  \n",
    "print(\"Accuracy of testing data = {:2.1f}%\".format(scores[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0iFkXtSCMgy"
   },
   "source": [
    "Finally, we would like to combine CNN and RNN for sequential modeling.\n",
    "We utilize CNN to reduce the length of sequence and the number of parameters, in order to make the RNN training much efficient and robust (less issue of long-term dependency).\n",
    "\n",
    "The basic idea is shown in the following figure.\n",
    "\n",
    "![1dcnn](https://raw.githubusercontent.com/ckbjimmy/2018_mlw/master/img/1dcnn.png)\n",
    "\n",
    "[Source] Kim, 2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H5H2_u-GlQ9K",
    "outputId": "d2b5b9d6-11a2-474c-8fc7-982b963d3054"
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "# CNN + LSTM\n",
    "clf = Sequential()\n",
    "clf.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "clf.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "clf.add(MaxPooling1D(pool_size=2))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(100))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(Dense(1, activation='sigmoid'))\n",
    "clf.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(clf.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "NQf4IqSlegvU",
    "outputId": "d5d1bcef-3e49-4a05-c098-d37eb9755153"
   },
   "outputs": [],
   "source": [
    "history = clf.fit(x=X_train, y=y_train,\n",
    "                    validation_split=0.2,  \n",
    "                    batch_size=200,\n",
    "                    epochs=10, \n",
    "                    verbose=1)\n",
    "\n",
    "show_train_history(history, 'accuracy', 'val_accuracy')\n",
    "show_train_history(history, 'loss', 'val_loss')\n",
    "\n",
    "scores = clf.evaluate(X_test, y_test)  \n",
    "print(\"Accuracy of testing data = {:2.1f}%\".format(scores[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MP25XTi7-F5"
   },
   "source": [
    "Except for the CNN, RNN and LSTM layers we used in the tutorial, you may also want to play with other RNN variants such as gated recurrent units (GRU), bidirectional RNN/LSTM/GRU.\n",
    "They are all available in Keras and all you need is computational resource.\n",
    "\n",
    "Deep neural network has many variants and many hyperparameters for fine-tuning.\n",
    "We are not going to explain all functions above.\n",
    "If you are interested in the details of the models and want to fine-tune them, please definitely go to [Keras document](https://keras.io/) for more information.\n",
    "\n",
    "We hope that this tutorial helps you understand the basic of deep neural network---feedforward neural network, CNN and RNN, and how to choose and use them in your clinical machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwAYzccT97E6"
   },
   "source": [
    "## Further reading\n",
    "\n",
    "### Theory and mathematics\n",
    "- [Deep learning book](http://www.deeplearningbook.org/)\n",
    "- [Stanford CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)\n",
    "- [Stanford CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n",
    "\n",
    "### Practical\n",
    "You may want to read the documents of TensorFlow, PyTorch, Keras, Scikit-learn, ... etc. to know how to use them.\n",
    "\n",
    "- [Tensorflow Programmer's Guide](https://www.tensorflow.org/programmers_guide/)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [Coursera Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "- [Google Machine Learning Crash Course\n",
    "](https://developers.google.com/machine-learning/crash-course/ml-intro)\n",
    "- [Coursera Machine Learning with TensorFlow on Google Cloud Platform Specialization](https://www.coursera.org/specializations/machine-learning-tensorflow-gcp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of nb3_nn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
