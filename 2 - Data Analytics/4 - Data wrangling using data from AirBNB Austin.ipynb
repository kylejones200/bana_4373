{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis in practice\n",
    "\n",
    "This notebook applies the concepts we learned in class to an real dataset about current lists on AirBNB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulate and clean data\n",
    "\n",
    "Data is often \"dirty\" and needs to be cleaned up before we can analyze it. Excel can do some of this but one limitation to Excel is that there is no audit trail for the transformations that have been applied to the data. This can lead to issues with reproducing the results later. It also leads to \"magic numbers\" which are values like `40` or `120` that appear in calculations but have no clear origin - they appear like magic and are hard to traceback to an origin. \n",
    "\n",
    "# Tutorial Overview\n",
    "In this module, we will:\n",
    "1. import a file into a DataFrame\n",
    "2. remove extra characters in a column and recast the column as numeric\n",
    "3. plot the data as boxplots and histograms\n",
    "4. pivot ant cross-tabulate the data\n",
    "5. apply statistical techniques like ANOVA and regression to explore relationships in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing pandas and numpy and reading in the dataset. Now we have a Pandas DataFrame with the data. You'll notice this throws an error. Pandas automatically sets the data type for the columns during the import.  Some of the columns are mixed which will create issues for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/Austin AirBNB data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see the DataFrame is large - it has 74 columns. \n",
    "# We won't need all of these so you could drop the columns you don't need. \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['name','listing_url', 'scrape_id']\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the column for price. We want to see how the values are distributed.\n",
    "\n",
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to plot 'price' using a boxplot and histogram. Why doesn't this work?\n",
    "df.boxplot(column=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this histograph showing us? Hint: this is not right :)\n",
    "df['price'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Pandas imported 'price', it determined this variable was a string, not a number because it included the dollar sign and commas. \n",
    "\n",
    "We need to remove \"$\" and \",\" and then recast this column as a number (floating point since it has decimals). We can quickly fix the issues with price and continue our analysis. It is very common for datasets to have issues like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes characters end up in the data. this removes the dollar sign from the price column. \n",
    "\n",
    "df['price'] = df['price'].str.replace('$', '')\n",
    "df['price'] = df['price'].str.replace(',', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the price as a number, we can make a boxplot. \n",
    "\n",
    "df.boxplot(column=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The boxplot is hard to read because there are so many extreme values. Let's focus just on the values less than  or equal to $300.\n",
    "\n",
    "df[df['price']<=300].boxplot(column=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can make a histogram of the same distribution. \n",
    "\n",
    "df[df['price']<=300]['price'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we might want to save the cleaned DataFrame as a CSV so we don't need to repeat those steps. We have cleaned the price column, now let's do some analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wonder if being part of the AirBNB \"superhost\" program is worth it. Let's look at the distribuition of rental prices separated by if the host is a superhost or not. \n",
    "\n",
    "df[df['price']<301].boxplot(column=['price'], by = 'host_is_superhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a pivot table to look at the difference in price between superhosts and non-superhosts. The dataset uses \"t\" and \"f\" but lets change them to boolean values\n",
    "bool_map = {'t': True, 'f': False}\n",
    "df['host_is_superhost'] = df['host_is_superhost'].map(bool_map)\n",
    "\n",
    "pd.pivot_table(df, values = 'price', columns = 'host_is_superhost', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use SciPy to test if the difference between superhosts/non-super hosts is statistically significant\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "stats.ttest_ind(df[df['host_is_superhost']==True]['price'], df[df['host_is_superhost']==False]['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There is  evidence to indicate that the true mean rental price is different between superhosts and non-superhosts. (We reject the null hypothesis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# room_type and Price\n",
    "\n",
    "Let's see if the kind of bed offered is correlated with price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at bed type and see if that makes a difference.\n",
    "\n",
    "df[df['price']<301].boxplot(column=['price'], by = 'room_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are more than two groups we need to use ANOVA instead of a t-test. The StatsModels library can help.\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "mod = ols('price ~ room_type', data=df).fit()\n",
    "                \n",
    "aov_table = sm.stats.anova_lm(mod, typ=2)\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When StatsModels does the ANOVA, it also does a linear regression. We can see the results by calling the summary method on the model\n",
    "\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "the room_type variable is statistically significant but the $R^2$ value is very low. If we want to predict 'price', we need to use other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amenities and Price\n",
    "\n",
    "Each property offers a variety of amenities. The data is hard to analyze, so let's clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe amenities would be useful. There is a column for amenities but it is a JSON object. Let's pull some values from the column.\n",
    "\n",
    "df['amenities'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this to a format we can use by making dummy variables for key terms. We want to do analysis with these columns, so we convert the boolean values to integers.\n",
    "\n",
    "We create the \"watch_words\" list to hold the amenities we want to examine. This list could be a single amenity or a long list. It is important that we convert the column to lowercase before searching for the watch word so we don't miss different spellings such as \"WIFI\", \"Wifi\", \"WiFi\", or \"wifi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_words = ['wifi',  'cable tv', 'pet', 'cat']\n",
    "\n",
    "for i in watch_words: \n",
    "    df[i] = df['amenities'].str.lower().str.contains(i)\n",
    "\n",
    "df[watch_words]=df[watch_words].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[watch_words].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have additional factors we can use for analysis. It looks like almost all properties have wifi and only a few allow pets or cats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs and/or Cats versus Price\n",
    "\n",
    "Let's use pivot tables to look at how the price changes for properties that allow dogs and/or cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do properties that allow dogs and/or cats have a higher price? Let's do a pivot table to see\n",
    "\n",
    "pd.pivot_table(df, values= 'price', columns = 'pet', index = 'cat', aggfunc=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many properties are there that allow dogs or cats? Let's change the aggfunc from mean to count. We also add a summary column called \"All\"\n",
    "\n",
    "pd.pivot_table(df, values= 'price', columns = 'pet', index = 'cat', aggfunc=\"count\", margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pivot gave us total count, but what we want to know is a percentage of the total. Since we don't care about price, we can use crosstab\n",
    "\n",
    "pd.crosstab(df['cat'], df['pet'], normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice. So less than 1% of the properties allow both pets and cats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Dogs and Cats versus Price\n",
    "\n",
    "We can look at the interaction effect of allowing both dogs and cats using a regression. We will use StatsModels for the regressions because the structure is easier to read and understand than Scikit-Learn's regression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see how dogs or cats effect price with a regression\n",
    "\n",
    "mod = ols('price ~ cat*pet', data=df).fit()\n",
    "                \n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This interaction effect is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "Let's try something more advanced. We have the description of each property. Let's seen if the sentiment of the description is correlated with price. We will use the \"Vader\" sentiment analysis library which is pre-trained for social media text and emojiis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#Add VADER metrics to dataframe\n",
    "df['desc'] = df['description'].astype(str)\n",
    "#Add VADER metrics to dataframe\n",
    "df['compound'] = [analyzer.polarity_scores(v)['compound'] for v in df['desc']]\n",
    "df['neg'] = [analyzer.polarity_scores(v)['neg'] for v in df['desc']]\n",
    "df['neu'] = [analyzer.polarity_scores(v)['neu'] for v in df['desc']]\n",
    "df['pos'] = [analyzer.polarity_scores(v)['pos'] for v in df['desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Sentiment Analysis Polarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have new features for sentiment, we can regress those against price to see if there is an association\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "mod = ols('price ~ compound + neg + neu + pos', data=df).fit()\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The only one that is statistically significant is the compound score. Let's drop the others and try again. The compound score ranges from [-1, 1]. Like a correlation value, the strength of the relationship increases as you approach the extremes.\n",
    "\n",
    "## Compound Polarity Score versus Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ols('price ~ compound', data=df).fit()\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "The coefficient is statistically significant. But it is still hard to interpret. A 0.1 unit increase in compound polarity score is associated with a $2.45 increase in price.\n",
    "\n",
    "While this method cannot impute causality, we can hypothesis that if the market is functioning appropriately and the prices reflect what people are willing/able to pay, then the tone of the description matters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "* Describe three examples when Pandas would be better than using Excel directly.\n",
    "\n",
    "# Further Reading\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "## Books\n",
    "* Python for Data Analysis, by William McKinney. http://shop.oreilly.com/product/0636920023784.do\n",
    "\n",
    "## APIs\n",
    "* Pandas. https://pandas.pydata.org/\n",
    "\n",
    "## Articles\n",
    "* Getting started with Pandas in 5 minutes, on Towards Data Science. https://medium.com/bhavaniravi/python-pandas-tutorial-92018da85a33\n",
    "* My Pandas Cheat Sheet, on Towards Data Science. https://towardsdatascience.com/my-python-pandas-cheat-sheet-746b11e44368    \n",
    "\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you worked with data from Fortune using Pandas. Specifically, you learned:\n",
    "1. import a file into a DataFrame\n",
    "2. remove extra characters in a column and recast the column as numeric\n",
    "3. plot the data as boxplots and histograms\n",
    "4. pivot ant cross-tabulate the data\n",
    "5. apply statistical techniques like ANOVA and regression to explore relationships in the data\n",
    "\n",
    "# Next\n",
    "\n",
    "In the next section, you will use Pandas to work with additional datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
